{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9eir4r3i9mO2"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade mesa\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtTPYphLTvTO"
   },
   "source": [
    "# Definiendo clase obst谩culo y grid (escenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vuZNLJLIsEkf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import math\n",
    "from mesa import Agent, Model\n",
    "from mesa.space import MultiGrid\n",
    "from queue import PriorityQueue\n",
    "\n",
    "# -------------------------\n",
    "# Helper: Comprobar si una celda est谩 libre de obst谩culos\n",
    "# -------------------------\n",
    "def cell_is_free(model, pos):\n",
    "    cell_agents = model.grid.get_cell_list_contents(pos)\n",
    "    for agent in cell_agents:\n",
    "        if isinstance(agent, Obstacle):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# -------------------------\n",
    "# Clase Obstacle\n",
    "# -------------------------\n",
    "class Obstacle(Agent):\n",
    "    \"\"\"Representa un obst谩culo est谩tico en el grid.\"\"\"\n",
    "    def __init__(self, model, x, y):\n",
    "        super().__init__(model)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def step(self):\n",
    "        pass  # No se mueve\n",
    "\n",
    "# -------------------------\n",
    "# Modelo de Grid\n",
    "# -------------------------\n",
    "class GridModel(Model):\n",
    "    \"\"\"Modelo que carga un grid de dimensiones dadas, coloca obst谩culos desde datos JSON y crea un agente seg煤n el tipo.\"\"\"\n",
    "    def __init__(self, width, height, goalx, goaly, obstacles_data, agent_type):\n",
    "        super().__init__()\n",
    "        self.grid = MultiGrid(width, height, torus=False)\n",
    "        self.obstacles = []\n",
    "\n",
    "        for obs in obstacles_data:\n",
    "            obstacle = Obstacle(self, obs[\"x\"], obs[\"y\"])\n",
    "            self.grid.place_agent(obstacle, (obs[\"x\"], obs[\"y\"]))\n",
    "            self.obstacles.append(obstacle)\n",
    "\n",
    "        if agent_type == \"reactivo\":\n",
    "            self.agent = Walle_reactivo(self, 0, 0, goalx, goaly)\n",
    "        elif agent_type == \"A*\":\n",
    "            self.agent = Walle_AStar(self, 0, 0, goalx, goaly)\n",
    "        elif agent_type == \"Q-Learning\":\n",
    "            self.agent = Walle_QLearning(self, 0, 0, goalx, goaly)\n",
    "        elif agent_type == \"Bayesiano\":\n",
    "            self.agent = Walle_Bayesiano(self, 0, 0, goalx, goaly)\n",
    "        else:\n",
    "            raise ValueError(\"Tipo de agente desconocido\")\n",
    "        self.grid.place_agent(self.agent, (0, 0))\n",
    "\n",
    "    def step(self):\n",
    "        self.agent.step()\n",
    "\n",
    "    def save_log(self, filename=\"simulation_log.json\"):\n",
    "        log_data = {\n",
    "            \"grid_size\": {\"width\": self.grid.width, \"height\": self.grid.height},\n",
    "            \"obstacles\": [{\"x\": obs.x, \"y\": obs.y} for obs in self.obstacles],\n",
    "            \"agent\": {\n",
    "                \"type\": type(self.agent).__name__,\n",
    "                \"spawnPosition\": {\"x\": self.agent.spawn_x, \"y\": self.agent.spawn_y},\n",
    "                \"goal\": {\"x\": self.agent.goalx, \"y\": self.agent.goaly},\n",
    "                \"path\": self.agent.path\n",
    "            }\n",
    "        }\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(log_data, file, indent=4)\n",
    "        print(f\"Simulation log saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o4gv_pTT1oS"
   },
   "source": [
    "# Definici贸n de Agentes (Reactivo, A*, Q-Learning, Bayesiano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yooCFmOMt8sA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Agente Reactivo\n",
    "# -------------------------\n",
    "class Walle_reactivo(Agent):\n",
    "    \"\"\"Se mueve aleatoriamente hasta alcanzar la meta o exceder el n煤mero m谩ximo de pasos.\"\"\"\n",
    "    def __init__(self, model, x, y, goalx, goaly):\n",
    "        super().__init__(model)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.spawn_x = x\n",
    "        self.spawn_y = y\n",
    "        self.goalx = goalx\n",
    "        self.goaly = goaly\n",
    "        self.steps_taken = 0\n",
    "        self.path = [{\"x\": x, \"y\": y}]\n",
    "\n",
    "    def step(self):\n",
    "        if (self.x, self.y) == (self.goalx, self.goaly):\n",
    "            print(f\"Agente reactivo lleg贸 a la meta ({self.x}, {self.y}) en {self.steps_taken} pasos!\")\n",
    "            return\n",
    "\n",
    "\n",
    "        moves = []\n",
    "        grid_width, grid_height = self.model.grid.width, self.model.grid.height\n",
    "\n",
    "        # Solo se agregan movimientos a celdas que est茅n dentro del grid y libres de obst谩culos.\n",
    "        if self.x < grid_width - 1 and cell_is_free(self.model, (self.x+1, self.y)):\n",
    "            moves.append((self.x+1, self.y))\n",
    "        if self.x > 0 and cell_is_free(self.model, (self.x-1, self.y)):\n",
    "            moves.append((self.x-1, self.y))\n",
    "        if self.y < grid_height - 1 and cell_is_free(self.model, (self.x, self.y+1)):\n",
    "            moves.append((self.x, self.y+1))\n",
    "        if self.y > 0 and cell_is_free(self.model, (self.x, self.y-1)):\n",
    "            moves.append((self.x, self.y-1))\n",
    "\n",
    "        if moves:\n",
    "            new_x, new_y = random.choice(moves)\n",
    "            self.model.grid.move_agent(self, (new_x, new_y))\n",
    "            self.x, self.y = new_x, new_y\n",
    "            self.steps_taken += 1\n",
    "            self.path.append({\"x\": self.x, \"y\": self.y})\n",
    "            print(f\"Reactivo se movi贸 a ({self.x}, {self.y})\")\n",
    "        else:\n",
    "            print(\"Agente reactivo ya no tiene movimientos!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo A* usa la ecuaci贸n matem谩tica para calcular la funci贸n de costo estimado:\n",
    "\n",
    "$$\n",
    "f(n) = g(n) + h(n)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $f(n)$ es el costo total estimado de llegar al objetivo desde el nodo $n$.\n",
    "- $g(n)$ es el costo real acumulado desde el nodo inicial hasta el nodo $n$.\n",
    "- $h(n)$ es la heur铆stica, que estima el costo desde $n$ hasta el objetivo.\n",
    "\n",
    "La heur铆stica utilizada para este agente es la distancia Manhattan:\n",
    "\n",
    "$$\n",
    "h(n) = |x_n - x_{goal}| + |y_n - y_{goal}|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EZdZ05duuFJu"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Agente A* (Basado en metas)\n",
    "# -------------------------\n",
    "class Walle_AStar(Agent):\n",
    "    \"\"\"Utiliza A* para calcular y seguir la ruta 贸ptima hacia la meta.\"\"\"\n",
    "    def __init__(self, model, x, y, goalx, goaly):\n",
    "        super().__init__(model)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.spawn_x = x\n",
    "        self.spawn_y = y\n",
    "        self.goalx = goalx\n",
    "        self.goaly = goaly\n",
    "        self.path = [{\"x\": x, \"y\": y}]\n",
    "        self.a_star_path = self.a_star_search((self.x, self.y), (self.goalx, self.goaly))\n",
    "        self.path_index = 0\n",
    "        self.reached_goal = False\n",
    "        self.steps = 0 \n",
    "\n",
    "    def a_star_search(self, start, goal):\n",
    "        def heuristic(a, b):\n",
    "            return abs(a[0]-b[0]) + abs(a[1]-b[1])\n",
    "\n",
    "        open_set = PriorityQueue()\n",
    "        open_set.put((0, start))\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        f_score = {start: heuristic(start, goal)}\n",
    "        grid_width, grid_height = self.model.grid.width, self.model.grid.height\n",
    "\n",
    "        while not open_set.empty():\n",
    "            _, current = open_set.get()\n",
    "            if current == goal:\n",
    "                path = []\n",
    "                while current in came_from:\n",
    "                    path.append(current)\n",
    "                    current = came_from[current]\n",
    "                path.reverse()\n",
    "                return path\n",
    "\n",
    "            x, y = current\n",
    "            neighbors = [(x+1,y), (x-1,y), (x,y+1), (x,y-1)]\n",
    "            valid_neighbors = []\n",
    "            for n in neighbors:\n",
    "                if 0 <= n[0] < grid_width and 0 <= n[1] < grid_height and cell_is_free(self.model, n):\n",
    "                    valid_neighbors.append(n)\n",
    "            for neighbor in valid_neighbors:\n",
    "                tentative_g = g_score[current] + 1\n",
    "                if neighbor not in g_score or tentative_g < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g\n",
    "                    f_score[neighbor] = tentative_g + heuristic(neighbor, goal)\n",
    "                    open_set.put((f_score[neighbor], neighbor))\n",
    "        return []\n",
    "\n",
    "    def step(self):\n",
    "        if self.reached_goal:\n",
    "            return  \n",
    "\n",
    "        if self.path_index < len(self.a_star_path):\n",
    "            next_move = self.a_star_path[self.path_index]\n",
    "            self.model.grid.move_agent(self, next_move)\n",
    "            self.x, self.y = next_move\n",
    "            self.path.append({\"x\": self.x, \"y\": self.y})\n",
    "            self.path_index += 1\n",
    "            self.steps += 1 \n",
    "            print(f\"Agente A* se movi贸 a ({self.x}, {self.y})\")\n",
    "\n",
    "            \n",
    "            if (self.x, self.y) == (self.goalx, self.goaly):\n",
    "                print(f\"Agente AStar lleg贸 a la meta ({self.x}, {self.y})! \")\n",
    "                print(f\"Total de pasos: {self.steps} 垛锔\")\n",
    "                self.reached_goal = True  # Stop moving\n",
    "        else:\n",
    "            if not self.reached_goal:\n",
    "                print(f\"Agente AStar no encontr贸 un camino ({self.goalx}, {self.goaly}). \")\n",
    "                self.reached_goal = True  # Stop repeating messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agente de aprendizaje por refuerzo Q-Learning\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $Q(s, a)$ es el valor Q actual para el estado $s$ y la acci贸n $a$.\n",
    "- $\\alpha$ es la tasa de aprendizaje (learning rate).\n",
    "- $r$ es la recompensa obtenida tras tomar la acci贸n $a$ en el estado $s$.\n",
    "- $\\gamma$ es el factor de descuento (discount factor).\n",
    "- $\\max_{a'} Q(s', a')$ es el valor Q m谩ximo para el siguiente estado $s'$.\n",
    "- $Q(s, a)$ es el valor actual antes de la actualizaci贸n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OhAmfhfvuKsh"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Agente Q-Learning\n",
    "# -------------------------\n",
    "import random\n",
    "\n",
    "class Walle_QLearning(Agent):\n",
    "    \"\"\"Agente Q-Learning con exploraci贸n adaptativa y memoria para evitar bucles.\"\"\"\n",
    "\n",
    "    def __init__(self, model, x, y, goalx, goaly, alpha=0.5, gamma=0.9, epsilon=0.2, epsilon_decay=0.99):\n",
    "        super().__init__(model)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.spawn_x = x\n",
    "        self.spawn_y = y\n",
    "        self.goalx = goalx\n",
    "        self.goaly = goaly\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # descontador\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.q_table = {}  # State -> {Action: Q-value}\n",
    "        self.path = [{\"x\": x, \"y\": y}]\n",
    "        self.visited = {}  \n",
    "        self.steps = 0\n",
    "        self.reached_goal = False \n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.x, self.y)\n",
    "\n",
    "    def possible_actions(self):\n",
    "        return self.possible_actions_at((self.x, self.y))\n",
    "\n",
    "    def possible_actions_at(self, state):\n",
    "        x, y = state\n",
    "        actions = []\n",
    "        grid_width, grid_height = self.model.grid.width, self.model.grid.height\n",
    "        if x < grid_width - 1 and cell_is_free(self.model, (x + 1, y)):\n",
    "            actions.append(\"DER\")\n",
    "        if x > 0 and cell_is_free(self.model, (x - 1, y)):\n",
    "            actions.append(\"IZQ\")\n",
    "        if y < grid_height - 1 and cell_is_free(self.model, (x, y + 1)):\n",
    "            actions.append(\"ARRIBA\")\n",
    "        if y > 0 and cell_is_free(self.model, (x, y - 1)):\n",
    "            actions.append(\"ABAJO\")\n",
    "        return actions\n",
    "\n",
    "    def choose_action(self):\n",
    "        state = self.get_state()\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = {a: 0 for a in self.possible_actions()}\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.possible_actions())\n",
    "        return max(self.q_table[state], key=self.q_table[state].get)\n",
    "\n",
    "    def take_action(self, action):\n",
    "        moves = {\n",
    "            \"DER\": (self.x + 1, self.y),\n",
    "            \"IZQ\": (self.x - 1, self.y),\n",
    "            \"ARRIBA\": (self.x, self.y + 1),\n",
    "            \"ABAJO\": (self.x, self.y - 1)\n",
    "        }\n",
    "        return moves[action]\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state):\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = {a: 0 for a in self.possible_actions_at(next_state)}\n",
    "\n",
    "        best_next_q = max(self.q_table[next_state].values()) if self.q_table[next_state] else 0\n",
    "        current_q = self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.alpha * (reward + self.gamma * best_next_q - current_q)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Ejecuta acci贸n y actualiza Q-table.\"\"\"\n",
    "        if self.reached_goal:\n",
    "            return \n",
    "        state = self.get_state()\n",
    "        action = self.choose_action()\n",
    "        new_x, new_y = self.take_action(action)\n",
    "        next_state = (new_x, new_y)\n",
    "        # Stop movement if goal reached\n",
    "        if (self.x, self.y) == (self.goalx, self.goaly):\n",
    "            if not hasattr(self, \"reached_goal\") or not self.reached_goal:\n",
    "                print(f\"Agente Q-Learning lleg贸 a la meta ({self.x}, {self.y})! \")\n",
    "                print(f\"Total de pasos {self.steps} 垛锔\")\n",
    "                self.reached_goal = True\n",
    "            return\n",
    "\n",
    "        # Track visits for penalty\n",
    "        self.visited[state] = self.visited.get(state, 0) + 1\n",
    "        visit_penalty = -0.5 * self.visited[state]  # Stronger penalty\n",
    "\n",
    "        if (0 <= new_x < self.model.grid.width and 0 <= new_y < self.model.grid.height and\n",
    "            cell_is_free(self.model, (new_x, new_y))):\n",
    "\n",
    "            self.model.grid.move_agent(self, (new_x, new_y))\n",
    "            self.x, self.y = new_x, new_y\n",
    "            self.path.append({\"x\": self.x, \"y\": self.y})\n",
    "            \n",
    "            self.steps += 1\n",
    "            \n",
    "            reward = 10 if (new_x, new_y) == (self.goalx, self.goaly) else -0.1 + visit_penalty\n",
    "            self.update_q(state, action, reward, next_state)\n",
    "\n",
    "            print(f\"Agente Q-learning se movi贸 a la {action} a ({self.x}, {self.y})\")\n",
    "\n",
    "        else:\n",
    "            self.update_q(state, action, -2, state)\n",
    "            print(\"Agente QLearning hizo un movimiento inv谩lido!\")\n",
    "\n",
    "        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agente Bayesiano\n",
    "\n",
    "El agente selecciona una acci贸n  con una probabilidad proporcional a su peso actual (), normalizando la suma de probabilidades:\n",
    "\n",
    "$$\n",
    "P(a) = \\frac{w(a)}{\\sum_{b \\in A} w(b)}\n",
    "$$\n",
    "\n",
    "Despu茅s de ejecutar una acci贸n, el agente ajusta la probabilidad () seg煤n el 茅xito o fracaso:\n",
    "\n",
    "$$\n",
    "w(a) \\leftarrow w(a) \\times f\n",
    "$$\n",
    "\n",
    "donde $f$ depende del resultado:\n",
    "\n",
    "- $f = 1.10$ si la acci贸n llev贸 al objetivo $\\text{goal_reached} = \\text{True}$,\n",
    "- $f = 1.05$ si la acci贸n tuvo 茅xito,\n",
    "- $f = 0.95$ si la acci贸n fall贸."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JXQCA6rPuN3E"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Agente Bayesiano\n",
    "# -------------------------\n",
    "import random\n",
    "\n",
    "class Walle_Bayesiano(Agent):\n",
    "    \"\"\"Agente bayesiano que toma decisiones basadas en un modelo probabil铆stico simple,\n",
    "    con memoria de visitas y condici贸n de meta.\"\"\"\n",
    "    def __init__(self, model, x, y, goalx, goaly):\n",
    "        super().__init__(model)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.spawn_x = x\n",
    "        self.spawn_y = y\n",
    "        self.goalx = goalx\n",
    "        self.goaly = goaly\n",
    "        self.path = [{\"x\": x, \"y\": y}]\n",
    "        self.action_probs = {\"RIGHT\": 0.25, \"LEFT\": 0.25, \"UP\": 0.25, \"DOWN\": 0.25}\n",
    "        self.visited = {(x, y): 1}\n",
    "        self.visited_threshold = 3\n",
    "        self.reached_goal = False\n",
    "        self.steps = 0\n",
    "\n",
    "    def take_action(self, action):\n",
    "        \"\"\"Devuelve las coordenadas destino seg煤n la acci贸n elegida.\"\"\"\n",
    "        moves = {\n",
    "            \"RIGHT\": (self.x + 1, self.y),\n",
    "            \"LEFT\": (self.x - 1, self.y),\n",
    "            \"UP\": (self.x, self.y + 1),\n",
    "            \"DOWN\": (self.x, self.y - 1)\n",
    "        }\n",
    "        return moves.get(action, (self.x, self.y))\n",
    "\n",
    "    def possible_actions(self):\n",
    "        \"\"\"Devuelve una lista de acciones posibles seg煤n el entorno y la memoria de visitas.\"\"\"\n",
    "        actions = []\n",
    "        grid_width, grid_height = self.model.grid.width, self.model.grid.height\n",
    "        possible = []\n",
    "        if self.x < grid_width - 1 and cell_is_free(self.model, (self.x + 1, self.y)):\n",
    "            possible.append(\"RIGHT\")\n",
    "        if self.x > 0 and cell_is_free(self.model, (self.x - 1, self.y)):\n",
    "            possible.append(\"LEFT\")\n",
    "        if self.y < grid_height - 1 and cell_is_free(self.model, (self.x, self.y + 1)):\n",
    "            possible.append(\"UP\")\n",
    "        if self.y > 0 and cell_is_free(self.model, (self.x, self.y - 1)):\n",
    "            possible.append(\"DOWN\")\n",
    "\n",
    "        # filtrar acciones que llevan a celdas ya visitadas en exceso\n",
    "        filtered_actions = []\n",
    "        for action in possible:\n",
    "            new_pos = self.take_action(action)\n",
    "            count = self.visited.get(new_pos, 0)\n",
    "            if count < self.visited_threshold:\n",
    "                filtered_actions.append(action)\n",
    "        return filtered_actions if filtered_actions else possible\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Elige una acci贸n de entre las posibles usando las probabilidades asociadas.\"\"\"\n",
    "        actions = self.possible_actions()\n",
    "        total_prob = sum(self.action_probs[a] for a in actions)\n",
    "        r = random.random() * total_prob\n",
    "        cumulative = 0\n",
    "        for a in actions:\n",
    "            cumulative += self.action_probs[a]\n",
    "            if r <= cumulative:\n",
    "                return a\n",
    "        return random.choice(actions)\n",
    "\n",
    "    def update_beliefs(self, action, success, goal_reached=False):\n",
    "        \"\"\"\n",
    "        Actualiza las probabilidades de acci贸n mediante una regla multiplicativa.\n",
    "        Se usa un factor mayor si se alcanza la meta.\n",
    "        \"\"\"\n",
    "        if success:\n",
    "            factor = 1.10 if goal_reached else 1.05\n",
    "        else:\n",
    "            factor = 0.95\n",
    "        self.action_probs[action] *= factor\n",
    "\n",
    "        total = sum(self.action_probs.values())\n",
    "        if total > 0:\n",
    "            for a in self.action_probs:\n",
    "                self.action_probs[a] /= total\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Ejecuta un paso de acci贸n y actualiza las creencias seg煤n el resultado.\"\"\"\n",
    "        if self.x == self.goalx and self.y == self.goaly:\n",
    "            if not self.reached_goal:\n",
    "                print(\"隆Meta alcanzada!\")\n",
    "                print(f\"N煤mero total de pasos: {self.steps} 垛锔\")\n",
    "                self.reached_goal = True\n",
    "            return\n",
    "\n",
    "        action = self.choose_action()\n",
    "        new_x, new_y = self.take_action(action)\n",
    "\n",
    "        if (0 <= new_x < self.model.grid.width and 0 <= new_y < self.model.grid.height and\n",
    "            cell_is_free(self.model, (new_x, new_y))):\n",
    "            self.model.grid.move_agent(self, (new_x, new_y))\n",
    "            success = True\n",
    "            self.x, self.y = new_x, new_y\n",
    "            self.path.append({\"x\": self.x, \"y\": self.y})\n",
    "            self.steps += 1 \n",
    "            pos = (self.x, self.y)\n",
    "            self.visited[pos] = self.visited.get(pos, 0) + 1\n",
    "            print(f\"Walle_Bayesiano moved {action} to ({self.x}, {self.y})\")\n",
    "            \n",
    "        else:\n",
    "            success = False\n",
    "            print(f\"Walle_Bayesiano attempted invalid move {action}!\")\n",
    "\n",
    "        if self.x == self.goalx and self.y == self.goaly:\n",
    "            self.update_beliefs(action, success=True, goal_reached=True)\n",
    "        else:\n",
    "            self.update_beliefs(action, success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escenarios\n",
    "- Grid 1: Obst谩culos simples en 5x5\n",
    "<img src = 'esc1.jpeg' width = 250>\n",
    "```\n",
    "obstacles_data = [\n",
    "    {\"x\": 2, \"y\": 2},\n",
    "    {\"x\": 1, \"y\": 3}\n",
    "]\n",
    "\n",
    "```\n",
    "- Grid 2: Obst谩culos en 10x10\n",
    "```\n",
    "obstacles_data = [\n",
    "    {\"x\": 0, \"y\": 1}, {\"x\": 0, \"y\": 2}, {\"x\": 0, \"y\": 3}, {\"x\": 0, \"y\": 4},\n",
    "    {\"x\": 0, \"y\": 5}, {\"x\": 0, \"y\": 6}, {\"x\": 0, \"y\": 8}, {\"x\": 0, \"y\": 9},\n",
    "    {\"x\": 1, \"y\": 6}, {\"x\": 1, \"y\": 9},\n",
    "    {\"x\": 2, \"y\": 0}, {\"x\": 2, \"y\": 2}, {\"x\": 2, \"y\": 3}, {\"x\": 2, \"y\": 4}, {\"x\": 2, \"y\": 9},\n",
    "    {\"x\": 3, \"y\": 0}, {\"x\": 3, \"y\": 2}, {\"x\": 3, \"y\": 6}, {\"x\": 3, \"y\": 8}, {\"x\": 3, \"y\": 9},\n",
    "    {\"x\": 4, \"y\": 0}, {\"x\": 4, \"y\": 4},  {\"x\": 4, \"y\": 9},\n",
    "    {\"x\": 5, \"y\": 0}, {\"x\": 5, \"y\": 2}, {\"x\": 5, \"y\": 9},\n",
    "    {\"x\": 6, \"y\": 0}, {\"x\": 6, \"y\": 2}, {\"x\": 6, \"y\": 3}, {\"x\": 6, \"y\": 5}, {\"x\": 6, \"y\": 6},\n",
    "    {\"x\": 7, \"y\": 0}, {\"x\": 7, \"y\": 3}, {\"x\": 7, \"y\": 5}, {\"x\": 7, \"y\": 9},\n",
    "    {\"x\": 8, \"y\": 0}, {\"x\": 8, \"y\": 7}, {\"x\": 8, \"y\": 9},\n",
    "    {\"x\": 9, \"y\": 0}, {\"x\": 9, \"y\": 1}, {\"x\": 9, \"y\": 2}, {\"x\": 9, \"y\": 3}, {\"x\": 9, \"y\": 4},\n",
    "    {\"x\": 9, \"y\": 5}, {\"x\": 9, \"y\": 6}, {\"x\": 9, \"y\": 7}, {\"x\": 9, \"y\": 8}, {\"x\": 9, \"y\": 9},\n",
    "]\n",
    "\n",
    "```\n",
    "<img src = 'esc2.png' width = 250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NC_YDwfyuSg2",
    "outputId": "b7aa8abf-8d37-456d-e92e-71da0f6cf0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walle_Bayesiano moved RIGHT to (1, 0)\n",
      "Walle_Bayesiano moved UP to (1, 1)\n",
      "Walle_Bayesiano moved RIGHT to (2, 1)\n",
      "Walle_Bayesiano moved RIGHT to (3, 1)\n",
      "Walle_Bayesiano moved LEFT to (2, 1)\n",
      "Walle_Bayesiano moved RIGHT to (3, 1)\n",
      "Walle_Bayesiano moved LEFT to (2, 1)\n",
      "Walle_Bayesiano moved RIGHT to (3, 1)\n",
      "Walle_Bayesiano moved RIGHT to (4, 1)\n",
      "Walle_Bayesiano moved RIGHT to (5, 1)\n",
      "Walle_Bayesiano moved LEFT to (4, 1)\n",
      "Walle_Bayesiano moved RIGHT to (5, 1)\n",
      "Walle_Bayesiano moved RIGHT to (6, 1)\n",
      "Walle_Bayesiano moved LEFT to (5, 1)\n",
      "Walle_Bayesiano moved LEFT to (4, 1)\n",
      "Walle_Bayesiano moved UP to (4, 2)\n",
      "Walle_Bayesiano moved UP to (4, 3)\n",
      "Walle_Bayesiano moved RIGHT to (5, 3)\n",
      "Walle_Bayesiano moved UP to (5, 4)\n",
      "Walle_Bayesiano moved UP to (5, 5)\n",
      "Walle_Bayesiano moved LEFT to (4, 5)\n",
      "Walle_Bayesiano moved RIGHT to (5, 5)\n",
      "Walle_Bayesiano moved DOWN to (5, 4)\n",
      "Walle_Bayesiano moved UP to (5, 5)\n",
      "Walle_Bayesiano moved DOWN to (5, 4)\n",
      "Walle_Bayesiano moved DOWN to (5, 3)\n",
      "Walle_Bayesiano moved LEFT to (4, 3)\n",
      "Walle_Bayesiano moved LEFT to (3, 3)\n",
      "Walle_Bayesiano moved RIGHT to (4, 3)\n",
      "Walle_Bayesiano moved RIGHT to (5, 3)\n",
      "Walle_Bayesiano moved LEFT to (4, 3)\n",
      "Walle_Bayesiano moved DOWN to (4, 2)\n",
      "Walle_Bayesiano moved DOWN to (4, 1)\n",
      "Walle_Bayesiano moved UP to (4, 2)\n",
      "Walle_Bayesiano moved UP to (4, 3)\n",
      "Walle_Bayesiano moved LEFT to (3, 3)\n",
      "Walle_Bayesiano moved UP to (3, 4)\n",
      "Walle_Bayesiano moved DOWN to (3, 3)\n",
      "Walle_Bayesiano moved UP to (3, 4)\n",
      "Walle_Bayesiano moved UP to (3, 5)\n",
      "Walle_Bayesiano moved DOWN to (3, 4)\n",
      "Walle_Bayesiano moved UP to (3, 5)\n",
      "Walle_Bayesiano moved LEFT to (2, 5)\n",
      "Walle_Bayesiano moved UP to (2, 6)\n",
      "Walle_Bayesiano moved UP to (2, 7)\n",
      "Walle_Bayesiano moved UP to (2, 8)\n",
      "Walle_Bayesiano moved DOWN to (2, 7)\n",
      "Walle_Bayesiano moved UP to (2, 8)\n",
      "Walle_Bayesiano moved LEFT to (1, 8)\n",
      "Walle_Bayesiano moved DOWN to (1, 7)\n",
      "Walle_Bayesiano moved RIGHT to (2, 7)\n",
      "Walle_Bayesiano moved DOWN to (2, 6)\n",
      "Walle_Bayesiano moved DOWN to (2, 5)\n",
      "Walle_Bayesiano moved RIGHT to (3, 5)\n",
      "Walle_Bayesiano moved RIGHT to (4, 5)\n",
      "Walle_Bayesiano moved UP to (4, 6)\n",
      "Walle_Bayesiano moved UP to (4, 7)\n",
      "Walle_Bayesiano moved LEFT to (3, 7)\n",
      "Walle_Bayesiano moved RIGHT to (4, 7)\n",
      "Walle_Bayesiano moved DOWN to (4, 6)\n",
      "Walle_Bayesiano moved DOWN to (4, 5)\n",
      "Walle_Bayesiano moved UP to (4, 6)\n",
      "Walle_Bayesiano moved UP to (4, 7)\n",
      "Walle_Bayesiano moved UP to (4, 8)\n",
      "Walle_Bayesiano moved RIGHT to (5, 8)\n",
      "Walle_Bayesiano moved LEFT to (4, 8)\n",
      "Walle_Bayesiano moved RIGHT to (5, 8)\n",
      "Walle_Bayesiano moved RIGHT to (6, 8)\n",
      "Walle_Bayesiano moved RIGHT to (7, 8)\n",
      "Walle_Bayesiano moved LEFT to (6, 8)\n",
      "Walle_Bayesiano moved LEFT to (5, 8)\n",
      "Walle_Bayesiano moved RIGHT to (6, 8)\n",
      "Walle_Bayesiano moved RIGHT to (7, 8)\n",
      "Walle_Bayesiano moved DOWN to (7, 7)\n",
      "Walle_Bayesiano moved UP to (7, 8)\n",
      "Walle_Bayesiano moved RIGHT to (8, 8)\n",
      "Walle_Bayesiano moved LEFT to (7, 8)\n",
      "Walle_Bayesiano moved RIGHT to (8, 8)\n",
      "Walle_Bayesiano moved LEFT to (7, 8)\n",
      "Walle_Bayesiano moved DOWN to (7, 7)\n",
      "Walle_Bayesiano moved DOWN to (7, 6)\n",
      "Walle_Bayesiano moved UP to (7, 7)\n",
      "Walle_Bayesiano moved LEFT to (6, 7)\n",
      "Walle_Bayesiano moved LEFT to (5, 7)\n",
      "Walle_Bayesiano moved DOWN to (5, 6)\n",
      "Walle_Bayesiano moved UP to (5, 7)\n",
      "Walle_Bayesiano moved DOWN to (5, 6)\n",
      "Walle_Bayesiano moved UP to (5, 7)\n",
      "Walle_Bayesiano moved RIGHT to (6, 7)\n",
      "Walle_Bayesiano moved UP to (6, 8)\n",
      "Walle_Bayesiano moved DOWN to (6, 7)\n",
      "Walle_Bayesiano moved UP to (6, 8)\n",
      "Walle_Bayesiano moved UP to (6, 9)\n",
      "隆Meta alcanzada!\n",
      "N煤mero total de pasos: 93 垛锔\n",
      "Simulation log saved to scenario_log.json\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Ejecuci贸n de la simulaci贸n\n",
    "# -------------------------\n",
    "\n",
    "grid_width = 10\n",
    "grid_height = 10\n",
    "goal_x = 6\n",
    "goal_y = 9\n",
    "obstacles_data = [\n",
    "    {\"x\": 0, \"y\": 1}, {\"x\": 0, \"y\": 2}, {\"x\": 0, \"y\": 3}, {\"x\": 0, \"y\": 4},\n",
    "    {\"x\": 0, \"y\": 5}, {\"x\": 0, \"y\": 6}, {\"x\": 0, \"y\": 8}, {\"x\": 0, \"y\": 9},\n",
    "    {\"x\": 1, \"y\": 6}, {\"x\": 1, \"y\": 9},\n",
    "    {\"x\": 2, \"y\": 0}, {\"x\": 2, \"y\": 2}, {\"x\": 2, \"y\": 3}, {\"x\": 2, \"y\": 4}, {\"x\": 2, \"y\": 9},\n",
    "    {\"x\": 3, \"y\": 0}, {\"x\": 3, \"y\": 2}, {\"x\": 3, \"y\": 6}, {\"x\": 3, \"y\": 8}, {\"x\": 3, \"y\": 9},\n",
    "    {\"x\": 4, \"y\": 0}, {\"x\": 4, \"y\": 4},  {\"x\": 4, \"y\": 9},\n",
    "    {\"x\": 5, \"y\": 0}, {\"x\": 5, \"y\": 2}, {\"x\": 5, \"y\": 9},\n",
    "    {\"x\": 6, \"y\": 0}, {\"x\": 6, \"y\": 2}, {\"x\": 6, \"y\": 3}, {\"x\": 6, \"y\": 5}, {\"x\": 6, \"y\": 6},\n",
    "    {\"x\": 7, \"y\": 0}, {\"x\": 7, \"y\": 3}, {\"x\": 7, \"y\": 5}, {\"x\": 7, \"y\": 9},\n",
    "    {\"x\": 8, \"y\": 0}, {\"x\": 8, \"y\": 7}, {\"x\": 8, \"y\": 9},\n",
    "    {\"x\": 9, \"y\": 0}, {\"x\": 9, \"y\": 1}, {\"x\": 9, \"y\": 2}, {\"x\": 9, \"y\": 3}, {\"x\": 9, \"y\": 4},\n",
    "    {\"x\": 9, \"y\": 5}, {\"x\": 9, \"y\": 6}, {\"x\": 9, \"y\": 7}, {\"x\": 9, \"y\": 8}, {\"x\": 9, \"y\": 9},\n",
    "]\n",
    "\n",
    "# tipo agente: \"reactivo\", \"A*\", \"Q-Learning\" o \"Bayesiano\"\n",
    "agent_type = \"Bayesiano\"\n",
    "\n",
    "model = GridModel(grid_width, grid_height, goal_x, goal_y, obstacles_data, agent_type=agent_type)\n",
    "\n",
    "for _ in range(1000):\n",
    "    model.step()\n",
    "\n",
    "model.save_log(\"scenario_log.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "oTVGzwEivYuA",
    "outputId": "805beba2-3b81-4c08-8190-2b2ebcf62d12"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e18dd2b1-877e-4a5a-82f2-51a572138532\", \"scenario_log.json\", 6828)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"scenario_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "webfoBm8S7Xq"
   },
   "source": [
    "# Ejemplo Unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QHnhoW3va57",
    "outputId": "68a86446-73cd-4ea2-f6e1-20151783b29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walle_reactivo moved to (0, 1)\n",
      "Walle_reactivo moved to (0, 0)\n",
      "Walle_reactivo moved to (1, 0)\n",
      "Walle_reactivo moved to (2, 0)\n",
      "Walle_reactivo moved to (1, 0)\n",
      "Walle_reactivo moved to (1, 1)\n",
      "Walle_reactivo moved to (1, 0)\n",
      "Walle_reactivo moved to (1, 1)\n",
      "Walle_reactivo moved to (0, 1)\n",
      "Walle_reactivo moved to (0, 2)\n",
      "Walle_reactivo moved to (0, 3)\n",
      "Walle_reactivo moved to (0, 2)\n",
      "Walle_reactivo moved to (0, 3)\n",
      "Walle_reactivo moved to (0, 2)\n",
      "Walle_reactivo moved to (0, 1)\n",
      "Walle_reactivo moved to (1, 1)\n",
      "Walle_reactivo moved to (2, 1)\n",
      "Walle_reactivo moved to (2, 0)\n",
      "Walle_reactivo moved to (1, 0)\n",
      "Walle_reactivo moved to (0, 0)\n",
      "Walle_reactivo moved to (0, 1)\n",
      "Walle_reactivo moved to (0, 2)\n",
      "Walle_reactivo moved to (0, 3)\n",
      "Walle_reactivo moved to (1, 3)\n",
      "Walle_reactivo moved to (2, 3)\n",
      "Walle_reactivo moved to (2, 2)\n",
      "Walle_reactivo moved to (1, 2)\n",
      "Walle_reactivo moved to (1, 3)\n",
      "Walle_reactivo moved to (0, 3)\n",
      "Walle_reactivo moved to (0, 2)\n",
      "Simulation log saved to walle_log.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from mesa import Agent, Model\n",
    "from mesa.space import MultiGrid #Implementamos los agentes\n",
    "\n",
    "class Walle_reactivo(Agent):\n",
    "    \"\"\"Agent that moves randomly until it reaches the goal or fails after max steps.\"\"\"\n",
    "\n",
    "    def __init__(self, model, x, y, goalx, goaly, max_steps=30):\n",
    "        super().__init__(model)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.spawn_x = x\n",
    "        self.spawn_y = y\n",
    "        self.goalx = goalx\n",
    "        self.goaly = goaly\n",
    "        self.steps_taken = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.path = []\n",
    "    def step(self):\n",
    "        \"\"\"Move randomly in x or y direction until reaching goal or exceeding max steps.\"\"\"\n",
    "        #Aqui vamos a poner las cosas que haremos cada paso, como imprimir el camino y revisar si ya nos\n",
    "        #Pasamos de pasos para detenernos\n",
    "\n",
    "        if (self.x, self.y) == (self.goalx, self.goaly):\n",
    "            print(f\"Walle_reactivo reached the goal at ({self.x}, {self.y}) in {self.steps_taken} steps!\")\n",
    "            return\n",
    "\n",
    "        #Justo aqui revisamos si ya nos pasamos de pasos\n",
    "        if self.steps_taken >= self.max_steps:\n",
    "            print(f\"Walle_reactivo FAILED to reach the goal within {self.max_steps} steps!\")\n",
    "            return\n",
    "\n",
    "        moves = []\n",
    "        #Percibimos el tama帽o del grid\n",
    "        grid_width, grid_height = self.model.grid.width, self.model.grid.height\n",
    "\n",
    "        # Revisamos si me puedo mover para esa direccion\n",
    "        if self.x < grid_width - 1:\n",
    "            moves.append((self.x + 1, self.y))\n",
    "        if self.x > 0:\n",
    "            moves.append((self.x - 1, self.y))\n",
    "        if self.y < grid_height - 1:\n",
    "            moves.append((self.x, self.y + 1))\n",
    "        if self.y > 0:\n",
    "            moves.append((self.x, self.y - 1))\n",
    "\n",
    "\n",
    "        if moves:\n",
    "            #Me muevo en una direccion random\n",
    "            new_x, new_y = random.choice(moves)\n",
    "            #Movemos el agente en el grid\n",
    "            self.model.grid.move_agent(self, (new_x, new_y))\n",
    "            #Actualizamos la posici贸n del agente\n",
    "            self.x, self.y = new_x, new_y\n",
    "            #Sumamos un paso\n",
    "            self.steps_taken += 1\n",
    "            #Guardo el paso que di\n",
    "            self.path.append({\"x\": self.x, \"y\": self.y})  # Save position in history\n",
    "            #Imprimo a donde me movi\n",
    "            print(f\"Walle_reactivo moved to ({self.x}, {self.y})\")\n",
    "\n",
    "class GridModel(Model):\n",
    "    \"\"\"A grid model containing reactive agents.\"\"\"\n",
    "\n",
    "    def __init__(self, width, height, goalx, goaly):\n",
    "        super().__init__()\n",
    "        self.grid = MultiGrid(width, height, torus=False)\n",
    "\n",
    "        # Creamos el agente reactivo y le digo que va a empezar en 0,0\n",
    "        #Con M谩ximo 25 pasos para llegar a la meta\n",
    "        self.agent1 = Walle_reactivo(self, 0, 0, goalx, goaly)\n",
    "        #Lo pongo en el grid en el inicio\n",
    "        self.grid.place_agent(self.agent1, (0, 0))\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Advance the model by one step.\"\"\"\n",
    "        #Caminamos un paso del agente, si tuviera mas agentes, aqui camino todos\n",
    "        #esos pasos\n",
    "\n",
    "        self.agent1.step()\n",
    "\n",
    "    def save_log(self, filename=\"walle_log.json\"):\n",
    "        \"\"\"Save movement history to a JSON file in the required format.\"\"\"\n",
    "        #Genero el JSON para UNITY en el formato que ya estamos usando en\n",
    "        #El test bed\n",
    "\n",
    "        log_data = {\n",
    "            \"robots\": [\n",
    "                {\n",
    "                    \"spawnPosition\": {\n",
    "                        \"x\": self.agent1.spawn_x,\n",
    "                        \"y\": self.agent1.spawn_y\n",
    "                    },\n",
    "                    \"path\": self.agent1.path\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(log_data, file, indent=4)\n",
    "        print(f\"Simulation log saved to {filename}\") #Corremos la solucion\n",
    "# -------------------------\n",
    "# RUNNING THE SIMULATION\n",
    "# -------------------------\n",
    "\n",
    "# Definimos la maya y la meta del agente\n",
    "GRID_WIDTH = 10\n",
    "GRID_HEIGHT = 10\n",
    "GOAL_X = 7\n",
    "GOAL_Y = 7\n",
    "\n",
    "# Creamos una instancia del modelo para poder ejecutarlo\n",
    "model = GridModel(GRID_WIDTH, GRID_HEIGHT, GOAL_X, GOAL_Y)\n",
    "\n",
    "# Corremos el modelo\n",
    "for _ in range(30):\n",
    "    #Aqu铆 andamos tomando los pasos, le pusimos 30 para apreciar que no termina\n",
    "    #En 25 pasos\n",
    "    model.step()\n",
    "\n",
    "# Guardamos los resultados en el log, y el log lo vemos\n",
    "# <- aca podras encontrar el archivo: walle_log.json\n",
    "model.save_log()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
